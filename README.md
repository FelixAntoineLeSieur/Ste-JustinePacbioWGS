# Ste-JustinePacbioWGS
Pacbio long read Whole Genome sequencing data processing, including Preanalysis, WGS Analysis and Post-analysis


We describe here a workflow for analyzing and processing data received from the Revio PacBio sequencer acquired by the CHU Sainte-Justine. 
Note that all of these scripts are meant to be run on the Digital Research Alliance of Canada clusters (Narval, Rorqual, Fir).

## Data processing
![Diag](https://github.com/user-attachments/assets/488dfa31-3974-43fd-ba15-68190e756015)
Once out of the sequencer, the long-read sequencing data is automatically transfered on a in-house smrtlink server, for temporary storage. 
Since storage and computing resources are limited on this server, we transfer the data to a cluster of the Alliance, [Narval](https://docs.alliancecan.ca/wiki/Narval).
\
From there, we pre-process the data with the scripts included here in [Preprocessing](#Pre-analysis). Then, the analysis itself is done using PacBio's [WGS pipeline](https://github.com/PacificBiosciences/HiFi-human-WGS-WDL). Note that I have my own fork of this pipeline [here](https://github.com/FelixAntoineLeSieur/HiFi-human-WGS-WDL), which should be included here as a submodule in [Analysis](#Analysis). It contains some minor changes aimed at making the pipeline usable in the Alliance environment.
\
Once the data has been processed by the WGS pipeline, we want to setup tertiary analysis, primarily through the GeneYX website, though different analysis and reports will likely be added later. 
This will described here in [Post-analysis](#post-analysis)

## Requirements
Using the scripts here requires access to GeneYX, Emedgene and Phenotips. Access information and paths must be defined in the config (by default named .myconf.json). \

**TODO** [The specific tool versions will be described here later] 

### Files and directories
My fork of the analysis pipeline is available [here](https://github.com/FelixAntoineLeSieur/HiFi-human-WGS-WDL) and includes installation instruction. \

We need the input files which are contained in the **run_path** given in the config. \
**run path** is a directory containing all the runs separated by run ID, as obtained by the Revio sequencer. This ID follows: r84196_{run_Start_Date}_{run_Start_Time}. Each of these folders contain subdirectories for each sequencing plate (1_A01, 1_B01...), each representing a single sample. Samples are further divided in subdirectories containing data related to the sample, the ones required by the scripts are in the subdirectories hifi_reads (contains the HiFi reads in unaligned BAM format), pb_format (contains metadata related to the sample, including the sample_name) \
**ref_path** is usually included in the HiFi-human-WGS-WDL pipeline as a template, in which the user must set paths to a resource bundle available here:\
[<img src="https://zenodo.org/badge/DOI/10.5281/zenodo.14027047.svg" alt="10.5281/zenodo.14027047">](https://zenodo.org/records/14027047) \
**tertiary_maps** is generated in a similar way to ref_path. \
**sample_sheet_path** is the link to the directory that will contain the pipeline samplesheets (in .json) and the batch files (in .txt).\
**Outputs** is unused for now, it will contain the outputs of the Analysis part \
**sample_list** is the CSV file that contains the metadata of samples retrieved by Preanalysis/getSamples.py.  All desired samples must be in this list before the sample sheets can be generated by Preanalysis/singletonSampleSheet or Preanalysis/jointCallSampleSheet.py See more about the format [here](#sample-list)\
**s3-folder** certain files are extracted to be sent to S3. For this we design a folder where we will send every file that must be copied to s3.
 


### Config
The config file follows this format see configTemplate.json:
```
{
	"Emedgene":
		{
			"username":"XXX",
			"password":"YYY",
			"endpoint":"https://chusaintejustine.emedgene.com"
		},
	"Phenotips":
		{
			"auth":"Basic XXX",
			"secret":"YYY",
			"endpoint":"https://chusj.phenotips.com"
		},
	"GeneYX":
		{
			"server": "https://analysis.geneyx.com",
			"apiUserId": "XXX",
			"apiUserKey": "YYY"
		},
	"Paths":
		{
			"run_path": "/home/felixant/projects/ctb-rallard/COMMUN/PacBioData/",
			"ref_maps": "/home/felixant/scratch/MINIWDL/HiFi-human-WGS-WDL/GRCh38.ref_map.v2p0p0.tsv",
			"sample_sheet_path": "/home/felixant/scratch/MINIWDL/SampleSheet/",
			"output_path": "/home/felixant/scratch/MINIWDL/Outputs",
			"tertiary_maps": "/home/felixant/scratch/MINIWDL/HiFi-human-WGS-WDL/GRCh38.tertiary_map.v2p0p0.tsv"
			"s3_folder": "/home/felixant/links/scratch/S3-Storage"
		},
	"Transfers":
		{
			"origin_cluster": "Fir",
			"origin_endpoint": "Globus UUID",
			"origin_collection": "Globus UUID",
			"identity_file": "/home/felixant/.ssh/FirInteractive",
			"destination_cluster": "Narval",
			"destination_path": "/home/felixant/projects/ctb-rallard/COMMUN/PacBioData/OutputFamilies/",
			"destination_endpoint": "Globus UUID",
			"destination_collection": "Globus UUID"
		}
}
```

Where the username and password are specified for each platform and the paths to the following are described in the [previous section](#files-and-directories)
The origin cluster is needed if we perform transfers of the processed output folder to another cluster. Usually, Narval is designed to hold the processed outputs.\
Also, if you need to send the data elsewhere, I highly recommend that you use the automation nodes, which you will need to create a ssh key for. The private key must be included in config to make use of the automation node. \ \
Instructions to use the automation nodes can be found [here](https://msss365-my.sharepoint.com/:p:/g/personal/nicolas_perrot_hsj_ssss_gouv_qc_ca/IQBk4eS7U82LS7RY-GDUzAotAb5FslgG8GfcCOw3VRsNF0s?e=3xbdHn )

### Sample List
The sample list gets built automatically when the script Preanalysis/getSamples.py gets used on a run ID. Data from all samples contained in that run are written in the list (by default named mySampleList.txt). The goal of this list is to save the information of samples from multiple runs without having to later reobtain the data from the Emedgene and Phenotips API. \
The information contained in the list is formatted like this (no header):
```
{sample_name},{plate_name},{runID},{Gender},{Singleton|Duo|Trio},{Proband|mother of {probandID}|father of {probandID}},{HPOList},{path_to_bam},{Affected? True|False}
GM1XXX;2_B01;2002;r84196_XXX;Male;Duo;proband;HP:0000XXX,HP:000YYY;{path_to_bam};True
GM2XXX;2_C01;2003;r84196_XXX;Female;Duo;mother of GM1XXX;;{path_to_bam};False
GM3XXX;2_D01;2004;r84196_YYY;Female;Duo;proband;HP:0000XXX;{path_to_bam};True
```
\
>[!NOTE]
>For the following, note that for most scripts you can use -c to supply an alternate config file name. Otherwise the default name is ".myconf.json".\
>Some scripts will also use default values for paths, such as "mySampleList.txt" for most Pre-analysis steps

## Pre-analysis
The goal of the pre-analysis is to obtain metadata for the samples contained in the desired run, then generate sample sheets to be used as inputs for the Analysis pipeline. \
The normal use of this part would normally be to use getSamples.py first on a recent run not yet in mySampleList.txt, then run *either* singletonSampleSheet.py on the desired samples names that should be run in singleton, OR jointCallSampleSheet.py to establish a family analysis. The getStudy.py is only used to get a reference of a study group for a line-separated list of samples. 
-	**getSamples.py**
	- *Usage*: ```python3 Preanalysis/getSamples.py -r {run_id}```  \
    - *Goal*: Use this script to fill to retrieve metadata from various sources and fill a metadata list so we don't have to retrieve that info every time. Precursor script to the samplesheet-writing scripts. \
	- *Outputs*: Using the script will print information about the samples contained in the run folder, like sample name, gender, family status and HPO terms. \
     The sample metadata will be added to a list defined as argument (by default mySampleList.txt). This list must not contain duplicate samples.
-	**getStudy.py**
	-	*Usage*: ```python3 Preanalysis/getStudy.py -s {line-separated list of ID you need the study group for} -p {pragmatiq csv extracted from sharepoint}``` \
  	-	*Goal*: Use this script to query a sample list extracted from sharepoint, the goal is to extract the study group (validation, Lr-prag, etc) of the samples given as a separate list \
    -	*Outputs*: This script simply writes out the study groups in stdout, I use this to fill my excel sheet.
- 	**singletonSampleSheet.py**
	- *Usage*: ```python3 Preanalysis/singletonSampleSheet.py -p {proband_name}``` \
 	- *Goal*: This is a samplesheet-writing script for singletons, requires the metadata list obtained with getSamples.py. Necessary for the Analysis phase \
	- *Outputs*: Will write two files in the configured **sample_sheet_path**:
		1. Pipeline sample sheet json named "{run_id}_{well}_{sample_name}.json
		2. "{run_id}_samples" which is a text file containing the list of all samples from this run for which the script singletonSampleSheet.py was used. This file will help keep all of these samples as a batch.
-	**jointCallSampleSheet.py**
	- *Usage*: ```python3 Preanalysis/jointCallSampleSheet.py -p {proband_name} -f {father_name} -m {mother_name} -n {chosen_family_name}``` \
    - *Goal*: This is a samplesheet-writing script for families, requires the metadata list obtained with getSamples.py. Necessary for the Analysis phase \
	- *Outputs*: Similar to the singleton SampleSheet, however for the family WDL has a different input samplesheet. Two files are generated in the configured **sample_sheet_path**:
		1. Pipeline sample sheet json named "{run_id}_{well}_{sample_name}.json
		2. "{run_id}_samples" which is a text file containing the list of all samples from this run for which the script singletonSampleSheet.py was used. This file will help keep all of these samples as a batch.

## Analysis
This section is about running the actual WGS pipeline, using the sample sheets created during the previous section. \ 
- **tmuxLaunchTrio.sh**
	- *Usage*: bash Analysis/tmuxLaunchTrio.py -i {Trio_ID}

## Post-analysis

